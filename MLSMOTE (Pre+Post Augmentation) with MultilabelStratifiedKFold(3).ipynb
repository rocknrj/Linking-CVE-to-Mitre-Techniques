{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5239942f",
   "metadata": {},
   "source": [
    "# Note:\n",
    "- If possible save MLSMOTE augmentation for both augmented and fixed datasets to a file that can be called seperately. This will help improve the speed when testing other models/performance evaluqtion strategies as MLSMOTE can take time balancing the datasets and repeating it for each code can be very time consuming.\n",
    "\n",
    "- The code is still being fixed. The current error is probably due to the n_sample and neigh values. One possibility is that n_samples (which was set to 100) generated too many synthetic samples, causing a fold to all be of one class. Changing these values could help fix this issue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3429e45",
   "metadata": {},
   "source": [
    "## MLSMOTE Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d43833c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from tqdm import tqdm\n",
    "\n",
    "def create_dataset(n_sample=1000):\n",
    "    ''' \n",
    "    Create an unevenly distributed sample data set multilabel  \n",
    "    classification using make_classification function\n",
    "    \n",
    "    args\n",
    "    nsample: int, Number of sample to be created\n",
    "    \n",
    "    return\n",
    "    X: pandas.DataFrame, feature vector dataframe with 10 features \n",
    "    y: pandas.DataFrame, target vector dataframe with 5 labels\n",
    "    '''\n",
    "    X, y = make_classification(n_classes=5, class_sep=2,\n",
    "                               weights=[0.1,0.025, 0.205, 0.008, 0.9], n_informative=3, n_redundant=1, flip_y=0,\n",
    "                               n_features=10, n_clusters_per_class=1, n_samples=1000, random_state=10)\n",
    "    y = pd.get_dummies(y, prefix='class')\n",
    "    return pd.DataFrame(X), y\n",
    "\n",
    "def get_tail_label(df: pd.DataFrame, ql=[0.05, 1.]) -> list:\n",
    "    \"\"\"\n",
    "    Find the underrepresented targets.\n",
    "    Underrepresented targets are those which are observed less than the median occurrence.\n",
    "    Targets beyond a quantile limit are filtered.\n",
    "    \"\"\"\n",
    "    irlbl = df.sum(axis=0)\n",
    "    irlbl = irlbl[(irlbl > irlbl.quantile(ql[0])) & ((irlbl < irlbl.quantile(ql[1])))]  # Filtering\n",
    "    irlbl = irlbl.max() / irlbl\n",
    "    threshold_irlbl = irlbl.median()\n",
    "    tail_label = irlbl[irlbl > threshold_irlbl].index.tolist()\n",
    "    return tail_label\n",
    "\n",
    "def get_minority_samples(X: pd.DataFrame, y: pd.DataFrame, ql=[0.05, 1.]):\n",
    "    \"\"\"\n",
    "    return\n",
    "    X_sub: pandas.DataFrame, the feature vector minority dataframe\n",
    "    y_sub: pandas.DataFrame, the target vector minority dataframe\n",
    "    \"\"\"\n",
    "    tail_labels = get_tail_label(y, ql=ql)\n",
    "    index = y[y[tail_labels].apply(lambda x: (x == 1).any(), axis=1)].index.tolist()\n",
    "    \n",
    "    X_sub = X[X.index.isin(index)].reset_index(drop=True)\n",
    "    y_sub = y[y.index.isin(index)].reset_index(drop=True)\n",
    "    return X_sub, y_sub\n",
    "\n",
    "def nearest_neighbour(X: pd.DataFrame, neigh) -> list:\n",
    "    \"\"\"\n",
    "    Give the index of 10 nearest neighbor of all the instances\n",
    "    \n",
    "    args\n",
    "    X: np.array, array whose nearest neighbor has to find\n",
    "    \n",
    "    return\n",
    "    indices: list of list, index of 5 NN of each element in X\n",
    "    \"\"\"\n",
    "    nbs = NearestNeighbors(n_neighbors=neigh, metric='euclidean', algorithm='kd_tree').fit(X)\n",
    "    euclidean, indices = nbs.kneighbors(X)\n",
    "    return indices\n",
    "\n",
    "def MLSMOTE(X, y, n_sample, neigh=5):\n",
    "    \"\"\"\n",
    "    Give the augmented data using MLSMOTE algorithm\n",
    "    \n",
    "    args\n",
    "    X: pandas.DataFrame, input vector DataFrame\n",
    "    y: pandas.DataFrame, feature vector dataframe\n",
    "    n_sample: int, number of newly generated sample\n",
    "    \n",
    "    return\n",
    "    new_X: pandas.DataFrame, augmented feature vector data\n",
    "    target: pandas.DataFrame, augmented target vector data\n",
    "    \"\"\"\n",
    "    indices2 = nearest_neighbour(X, neigh=5)\n",
    "    n = len(indices2)\n",
    "    new_X = np.zeros((n_sample, X.shape[1]))\n",
    "    target = np.zeros((n_sample, y.shape[1]))\n",
    "    \n",
    "    # Track progress with tqdm\n",
    "    progress_step = n_sample // 10  # Adjust the step as needed\n",
    "    progress_count = 0\n",
    "    \n",
    "    for i in range(n_sample):\n",
    "        reference = random.randint(0, n-1)\n",
    "        neighbor = random.choice(indices2[reference, 1:])\n",
    "        all_point = indices2[reference]\n",
    "        nn_df = y[y.index.isin(all_point)]\n",
    "        ser = nn_df.sum(axis=0, skipna=True)\n",
    "        target[i] = np.array([1 if val > 0 else 0 for val in ser])\n",
    "        ratio = random.random()\n",
    "        gap = X.loc[reference, :] - X.loc[neighbor, :]\n",
    "        new_X[i] = np.array(X.loc[reference, :] + ratio * gap)\n",
    "        \n",
    "        # Log progress\n",
    "        progress_count += 1\n",
    "        if progress_count % progress_step == 0:\n",
    "            print(f\"MLSOTE Progress: {progress_count}/{n_sample} samples generated\")\n",
    "\n",
    "    new_X = pd.DataFrame(new_X, columns=X.columns)\n",
    "    target = pd.DataFrame(target, columns=y.columns)\n",
    "    return new_X, target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b958054",
   "metadata": {},
   "source": [
    "## MLSMOTE on Augmented Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f424c8c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLSOTE Progress: 10/100 samples generated\n",
      "MLSOTE Progress: 20/100 samples generated\n",
      "MLSOTE Progress: 30/100 samples generated\n",
      "MLSOTE Progress: 40/100 samples generated\n",
      "MLSOTE Progress: 50/100 samples generated\n",
      "MLSOTE Progress: 60/100 samples generated\n",
      "MLSOTE Progress: 70/100 samples generated\n",
      "MLSOTE Progress: 80/100 samples generated\n",
      "MLSOTE Progress: 90/100 samples generated\n",
      "MLSOTE Progress: 100/100 samples generated\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0.0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1041295/817664822.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0mchain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClassifierChain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogistic_regression_classifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'random'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m     \u001b[0mchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_augmented\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_augmented\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;31m# Make predictions on the test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.10/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1149\u001b[0m                 )\n\u001b[1;32m   1150\u001b[0m             ):\n\u001b[0;32m-> 1151\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.10/site-packages/sklearn/multioutput.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, Y, **fit_params)\u001b[0m\n\u001b[1;32m    926\u001b[0m             )\n\u001b[1;32m    927\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 928\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    929\u001b[0m         self.classes_ = [\n\u001b[1;32m    930\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mchain_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.10/site-packages/sklearn/multioutput.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, Y, **fit_params)\u001b[0m\n\u001b[1;32m    724\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morder_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchain_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Chain\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 726\u001b[0;31m                 estimator.fit(\n\u001b[0m\u001b[1;32m    727\u001b[0m                     \u001b[0mX_aug\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mchain_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m                     \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.10/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1149\u001b[0m                 )\n\u001b[1;32m   1150\u001b[0m             ):\n\u001b[0;32m-> 1151\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1250\u001b[0m         \u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1251\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_classes\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1252\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   1253\u001b[0m                 \u001b[0;34m\"This solver needs samples of at least 2 classes\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m                 \u001b[0;34m\" in the data, but the data contains only one\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0.0"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multioutput import ClassifierChain\n",
    "from sklearn.metrics import hamming_loss, precision_recall_fscore_support, label_ranking_loss, accuracy_score\n",
    "\n",
    "# Load your data and preprocess it as before\n",
    "df = pd.read_excel('augmented_data2.xlsx')\n",
    "\n",
    "def lister(x):\n",
    "    return [x]\n",
    "\n",
    "df['technique_id'] = df['technique_id'].astype(str).tolist()\n",
    "df['description'] = df['description'].astype(str)\n",
    "X = df['description']\n",
    "y = df['technique_id']\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(X)\n",
    "\n",
    "# Split techniques into separate labels\n",
    "y = [technique.split(',') for technique in y]\n",
    "\n",
    "# Convert label column into a binary matrix\n",
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform(y)\n",
    "\n",
    "# Set aside 200 data points for later evaluation\n",
    "X_train_cv, X_eval, y_train_cv, y_eval = train_test_split(\n",
    "    X, y, test_size=800, random_state=42)\n",
    "\n",
    "# Define the number of splits for multi-label stratified cross-validation\n",
    "n_splits = 10\n",
    "\n",
    "# Initialize lists to store evaluation metrics\n",
    "precision_micro_values = []\n",
    "recall_micro_values = []\n",
    "f1_micro_values = []\n",
    "precision_macro_values = []\n",
    "recall_macro_values = []\n",
    "f1_macro_values = []\n",
    "hamming_loss_values = []\n",
    "ranking_loss_values = []\n",
    "accuracy_values = []\n",
    "\n",
    "# Create an instance of MultilabelStratifiedKFold\n",
    "mskf = MultilabelStratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform multi-label stratified cross-validation\n",
    "for train_indices, test_indices in mskf.split(X_train_cv, y_train_cv):\n",
    "    X_train, X_test = X_train_cv[train_indices], X_train_cv[test_indices]\n",
    "    y_train, y_test = y_train_cv[train_indices], y_train_cv[test_indices]\n",
    "\n",
    "    # Convert NumPy arrays to Pandas DataFrames\n",
    "    X_train_df = pd.DataFrame(X_train.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "    y_train_df = pd.DataFrame(y_train, columns=mlb.classes_)\n",
    "\n",
    "    # Apply MLSMOTE to augment the training data\n",
    "    X_train_augmented, y_train_augmented = MLSMOTE(X_train_df, y_train_df, n_sample=100, neigh=20)\n",
    "    \n",
    "    # Check if there are at least two classes in the augmented data\n",
    "    unique_classes = np.unique(y_train_augmented)\n",
    "    if len(unique_classes) < 2:\n",
    "        print(\"Skipping this iteration due to insufficient classes after MLSMOTE augmentation.\")\n",
    "        continue\n",
    "\n",
    "    # Create and train your classifier (e.g., 'chain' classifier)\n",
    "    logistic_regression_params = {\n",
    "        'solver': 'newton-cg',\n",
    "        'C': 5\n",
    "    }\n",
    "    logistic_regression_classifier = LogisticRegression(**logistic_regression_params)\n",
    "    \n",
    "    chain = ClassifierChain(logistic_regression_classifier, order='random')\n",
    "    chain.fit(X_train_augmented, y_train_augmented)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    scores2 = chain.predict(X_test)\n",
    "\n",
    "    # Calculate evaluation metrics for this fold\n",
    "    precision_micro, recall_micro, f1_micro, _ = precision_recall_fscore_support(y_test, scores2, average='micro')\n",
    "    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(y_test, scores2, average='macro')\n",
    "    hamming_loss_value = hamming_loss(y_test, scores2)\n",
    "    ranking_loss_value = label_ranking_loss(y_test, scores2)\n",
    "    accuracy_value = accuracy_score(y_test, scores2)\n",
    "\n",
    "    # Append metrics to the respective lists\n",
    "    precision_micro_values.append(precision_micro)\n",
    "    recall_micro_values.append(recall_micro)\n",
    "    f1_micro_values.append(f1_micro)\n",
    "    precision_macro_values.append(precision_macro)\n",
    "    recall_macro_values.append(recall_macro)\n",
    "    f1_macro_values.append(f1_macro)\n",
    "    hamming_loss_values.append(hamming_loss_value)\n",
    "    ranking_loss_values.append(ranking_loss_value)\n",
    "    accuracy_values.append(accuracy_value)\n",
    "\n",
    "\n",
    "# Calculate the mean of the evaluation metrics\n",
    "mean_precision_micro = np.mean(precision_micro_values)\n",
    "mean_recall_micro = np.mean(recall_micro_values)\n",
    "mean_f1_micro = np.mean(f1_micro_values)\n",
    "mean_precision_macro = np.mean(precision_macro_values)\n",
    "mean_recall_macro = np.mean(recall_macro_values)\n",
    "mean_f1_macro = np.mean(f1_macro_values)\n",
    "mean_hamming_loss = np.mean(hamming_loss_values)\n",
    "mean_ranking_loss = np.mean(ranking_loss_values)\n",
    "mean_accuracy = np.mean(accuracy_values)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Micro Precision:\", mean_precision_micro)\n",
    "print(\"Micro Recall:\", mean_recall_micro)\n",
    "print(\"Micro F1 Score:\", mean_f1_micro)\n",
    "print(\"Macro Precision:\", mean_precision_macro)\n",
    "print(\"Macro Recall:\", mean_recall_macro)\n",
    "print(\"Macro F1 Score:\", mean_f1_macro)\n",
    "print(\"Hamming Loss:\", mean_hamming_loss)\n",
    "print(\"Ranking Loss:\", mean_ranking_loss)\n",
    "print(\"Accuracy:\", mean_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9215fcbd",
   "metadata": {},
   "source": [
    "## MLSMOTE on Fixed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc651cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLSOTE Progress: 2/25 samples generated\n",
      "MLSOTE Progress: 4/25 samples generated\n",
      "MLSOTE Progress: 6/25 samples generated\n",
      "MLSOTE Progress: 8/25 samples generated\n",
      "MLSOTE Progress: 10/25 samples generated\n",
      "MLSOTE Progress: 12/25 samples generated\n",
      "MLSOTE Progress: 14/25 samples generated\n",
      "MLSOTE Progress: 16/25 samples generated\n",
      "MLSOTE Progress: 18/25 samples generated\n",
      "MLSOTE Progress: 20/25 samples generated\n",
      "MLSOTE Progress: 22/25 samples generated\n",
      "MLSOTE Progress: 24/25 samples generated\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0.0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1041295/1287727174.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0mchain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClassifierChain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogistic_regression_classifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'random'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m     \u001b[0mchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_augmented\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_augmented\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;31m# Make predictions on the test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.10/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1149\u001b[0m                 )\n\u001b[1;32m   1150\u001b[0m             ):\n\u001b[0;32m-> 1151\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.10/site-packages/sklearn/multioutput.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, Y, **fit_params)\u001b[0m\n\u001b[1;32m    926\u001b[0m             )\n\u001b[1;32m    927\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 928\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    929\u001b[0m         self.classes_ = [\n\u001b[1;32m    930\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mchain_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.10/site-packages/sklearn/multioutput.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, Y, **fit_params)\u001b[0m\n\u001b[1;32m    724\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morder_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchain_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Chain\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 726\u001b[0;31m                 estimator.fit(\n\u001b[0m\u001b[1;32m    727\u001b[0m                     \u001b[0mX_aug\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mchain_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m                     \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.10/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1149\u001b[0m                 )\n\u001b[1;32m   1150\u001b[0m             ):\n\u001b[0;32m-> 1151\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1250\u001b[0m         \u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1251\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_classes\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1252\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   1253\u001b[0m                 \u001b[0;34m\"This solver needs samples of at least 2 classes\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m                 \u001b[0;34m\" in the data, but the data contains only one\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0.0"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multioutput import ClassifierChain\n",
    "from sklearn.metrics import hamming_loss, precision_recall_fscore_support, label_ranking_loss, accuracy_score\n",
    "\n",
    "# Load your data and preprocess it as before\n",
    "df = pd.read_excel('Updated ENISA EXTRACTED2.xlsx')\n",
    "\n",
    "def lister(x):\n",
    "    return [x]\n",
    "\n",
    "df['technique_id'] = df['technique_id'].astype(str).tolist()\n",
    "df['description'] = df['description'].astype(str)\n",
    "X = df['description']\n",
    "y = df['technique_id']\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(X)\n",
    "\n",
    "# Split techniques into separate labels\n",
    "y = [technique.split(',') for technique in y]\n",
    "\n",
    "# Convert label column into a binary matrix\n",
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform(y)\n",
    "\n",
    "# Set aside 200 data points for later evaluation\n",
    "X_train_cv, X_eval, y_train_cv, y_eval = train_test_split(\n",
    "    X, y, test_size=200, random_state=42)\n",
    "\n",
    "# Define the number of splits for multi-label stratified cross-validation\n",
    "n_splits = 10\n",
    "\n",
    "# Initialize lists to store evaluation metrics\n",
    "precision_micro_values = []\n",
    "recall_micro_values = []\n",
    "f1_micro_values = []\n",
    "precision_macro_values = []\n",
    "recall_macro_values = []\n",
    "f1_macro_values = []\n",
    "hamming_loss_values = []\n",
    "ranking_loss_values = []\n",
    "accuracy_values = []\n",
    "\n",
    "# Create an instance of MultilabelStratifiedKFold\n",
    "mskf = MultilabelStratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform multi-label stratified cross-validation\n",
    "for train_indices, test_indices in mskf.split(X_train_cv, y_train_cv):\n",
    "    X_train, X_test = X_train_cv[train_indices], X_train_cv[test_indices]\n",
    "    y_train, y_test = y_train_cv[train_indices], y_train_cv[test_indices]\n",
    "\n",
    "    # Convert NumPy arrays to Pandas DataFrames\n",
    "    X_train_df = pd.DataFrame(X_train.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "    y_train_df = pd.DataFrame(y_train, columns=mlb.classes_)\n",
    "\n",
    "    # Apply MLSMOTE to augment the training data\n",
    "    X_train_augmented, y_train_augmented = MLSMOTE(X_train_df, y_train_df, n_sample=25, neigh=5)\n",
    "    \n",
    "    # Check if there are at least two classes in the augmented data\n",
    "    unique_classes = np.unique(y_train_augmented)\n",
    "    if len(unique_classes) < 2:\n",
    "        print(\"Skipping this iteration due to insufficient classes after MLSMOTE augmentation.\")\n",
    "        continue\n",
    "\n",
    "    # Create and train your classifier (e.g., 'chain' classifier)\n",
    "    logistic_regression_params = {\n",
    "        'solver': 'newton-cg',\n",
    "        'C': 5\n",
    "    }\n",
    "    logistic_regression_classifier = LogisticRegression(**logistic_regression_params)\n",
    "    \n",
    "    chain = ClassifierChain(logistic_regression_classifier, order='random')\n",
    "    chain.fit(X_train_augmented, y_train_augmented)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    scores2 = chain.predict(X_test)\n",
    "\n",
    "    # Calculate evaluation metrics for this fold\n",
    "    precision_micro, recall_micro, f1_micro, _ = precision_recall_fscore_support(y_test, scores2, average='micro')\n",
    "    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(y_test, scores2, average='macro')\n",
    "    hamming_loss_value = hamming_loss(y_test, scores2)\n",
    "    ranking_loss_value = label_ranking_loss(y_test, scores2)\n",
    "    accuracy_value = accuracy_score(y_test, scores2)\n",
    "\n",
    "    # Append metrics to the respective lists\n",
    "    precision_micro_values.append(precision_micro)\n",
    "    recall_micro_values.append(recall_micro)\n",
    "    f1_micro_values.append(f1_micro)\n",
    "    precision_macro_values.append(precision_macro)\n",
    "    recall_macro_values.append(recall_macro)\n",
    "    f1_macro_values.append(f1_macro)\n",
    "    hamming_loss_values.append(hamming_loss_value)\n",
    "    ranking_loss_values.append(ranking_loss_value)\n",
    "    accuracy_values.append(accuracy_value)\n",
    "\n",
    "# Calculate the mean of the evaluation metrics\n",
    "mean_precision_micro = np.mean(precision_micro_values)\n",
    "mean_recall_micro = np.mean(recall_micro_values)\n",
    "mean_f1_micro = np.mean(f1_micro_values)\n",
    "mean_precision_macro = np.mean(precision_macro_values)\n",
    "mean_recall_macro = np.mean(recall_macro_values)\n",
    "mean_f1_macro = np.mean(f1_macro_values)\n",
    "mean_hamming_loss = np.mean(hamming_loss_values)\n",
    "mean_ranking_loss = np.mean(ranking_loss_values)\n",
    "mean_accuracy = np.mean(accuracy_values)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Micro Precision:\", mean_precision_micro)\n",
    "print(\"Micro Recall:\", mean_recall_micro)\n",
    "print(\"Micro F1 Score:\", mean_f1_micro)\n",
    "print(\"Macro Precision:\", mean_precision_macro)\n",
    "print(\"Macro Recall:\", mean_recall_macro)\n",
    "print(\"Macro F1 Score:\", mean_f1_macro)\n",
    "print(\"Hamming Loss:\", mean_hamming_loss)\n",
    "print(\"Ranking Loss:\", mean_ranking_loss)\n",
    "print(\"Accuracy:\", mean_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aca67c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
